{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasmeen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "remove_these = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_STOPWORDS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lyrics/mxm_dataset_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lyrics/mxm_dataset_train.txt'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('lyrics/mxm_dataset_train.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    words = lines[17].replace('%','').split(',')\n",
    "    all_songs_dict = dict()\n",
    "    for i,l in list(enumerate(lines))[18:]:\n",
    "        song_info = l.split(',')\n",
    "        MSDID = song_info[0]\n",
    "        song_bow = [x.split(':') for x in song_info[2:]]\n",
    "        song_dict = {}\n",
    "        for word, word_count in song_bow:\n",
    "            song_dict[int(word)] = int(word_count.replace('\\n',''))\n",
    "        word_lists = [[words[word-1]]*song_dict[word] for word in song_dict.keys()]\n",
    "        song = [word for word_list in word_lists for word in word_list]\n",
    "        if REMOVE_STOPWORDS:\n",
    "            song = [w for w in song if w not in remove_these]\n",
    "        all_songs_dict[str(MSDID)] = ' '.join(song).replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_songs_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mall_songs_dict\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m      2\u001b[0m song_msd_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_songs_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_songs_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(all_songs_dict.keys()))\n",
    "song_msd_ids = list(all_songs_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.choice(song_msd_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_song_meta_dict = dict()\n",
    "with open('lyrics/mxm_779k_matches.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in range(18, len(lines)):\n",
    "        line = lines[i].split('<SEP>')\n",
    "        MSDID = line[0]\n",
    "        artist = line[1]\n",
    "        title = line[2]\n",
    "        all_song_meta_dict[str(MSDID)] = {'artist': artist, 'title': title}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_song_meta_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_artists = sorted(set([all_song_meta_dict[msdId]['artist'].lower() for msdId in all_song_meta_dict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'MSDID': list(all_songs_dict.keys()),\n",
    "    'cleaned_text': [all_songs_dict[x] for x in all_songs_dict.keys()]\n",
    "    }\n",
    "msdid_df = pd.DataFrame.from_dict(d)\n",
    "print(msdid_df.shape)\n",
    "msdid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'MSDID': msdid_df['MSDID'],\n",
    "    'artist': [all_song_meta_dict[x]['artist'] for x in msdid_df['MSDID']],\n",
    "    'title': [all_song_meta_dict[x]['title'] for x in msdid_df['MSDID']]\n",
    "    }\n",
    "meta_df = pd.DataFrame.from_dict(d)\n",
    "print(meta_df.shape)\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df\n",
    "full_df_ = pd.merge(msdid_df, meta_df, on='MSDID', how='left')\n",
    "print(full_df_.shape)\n",
    "full_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_artists_ = ['Green Day','Blink-182','Sum 41','Rise Against',\n",
    "              'Four Year Strong','Three Days Grace',\n",
    "              'Tame Impala','Snoop Dogg','Thin Lizzy','The Strokes',\n",
    "              'Nirvana','All Time Low', 'Foo Fighters','Lil Wayne',\n",
    "              'Eminem','Tom Petty','Seether', 'Shinedown',\n",
    "              'Red Hot Chili Peppers','The Who', 'The Doors',\n",
    "              'Linkin Park','Arctic Monkeys','The Black Keys',\n",
    "              'Led Zeppelin','The Kinks','Collective Soul',\n",
    "              'Avenged Sevenfold','George Harrison','Eric Clapton',\n",
    "              'The Donnas','System of a Down', 'MxPx',\n",
    "              'Weezer','Rancid','Atmosphere','Santigold',\n",
    "              'Iron Maiden','Fall Out Boy', 'Hank Williams']\n",
    "\n",
    "my_artists = ['The Smashing Pumpkins', 'Bruce Springsteen','Beastie Boys']\n",
    "\n",
    "sampled_artists = list(np.random.choice(full_df_['artist'], size=500, replace=False))\n",
    "# sampled_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_artists = full_df_['artist'].value_counts()[:10].reset_index()['index'].values\n",
    "# full_df = full_df_[full_df_['artist'].isin(my_artists)].reset_index(drop=True)\n",
    "# full_df = full_df_[full_df_['artist'].isin(sampled_artists)].reset_index(drop=True)\n",
    "# full_df = full_df_[full_df_['artist'].isin(sampled_artists + my_artists + my_artists_)].reset_index(drop=True)\n",
    "full_df = full_df_.copy()\n",
    "print(full_df.shape)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "full_df\n",
    "# get training data\n",
    "data = full_df['cleaned_text'].tolist()\n",
    "# test_data = test_df['cleaned_text'].tolist()\n",
    "train_corpus = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in list(enumerate(data))]\n",
    "# build model\n",
    "model = Doc2Vec(vector_size=50, min_count=1, epochs=10, dm=0)\n",
    "model.build_vocab(train_corpus)\n",
    "# train model\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_model_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('trained_model_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.DataFrame([model.docvecs[f'{i}'] for i in range(len(full_df))])\n",
    "fe_df = full_df.copy()\n",
    "for c in emb_df.columns:\n",
    "    fe_df[c] = emb_df[c]\n",
    "# fe_df[fe_df['artist'].isin(my_artists)].groupby('artist').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the fe_df with spotify dataset on artist and title\n",
    "\n",
    "spotify_df = pd.read_csv('spotify_data/train.csv')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.lower()\n",
    "spotify_df['name'] = spotify_df['name'].str.lower()\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace(' ','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace(' ','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('.','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('.','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('\\'','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('\\'','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('[','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('[','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace(']','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace(']','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('-','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('-','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('\\'','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('\\'','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('!','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('!','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('(','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('(','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace(')','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace(')','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace(',','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace(',','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('&','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('&','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('/','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('/','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('feat','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('feat','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('featuring','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('featuring','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('ft','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('ft','')\n",
    "spotify_df['artists'] = spotify_df['artists'].str.replace('ft.','')\n",
    "spotify_df['name'] = spotify_df['name'].str.replace('ft.','')\n",
    "\n",
    "\n",
    "#merge now \n",
    "fe_df['artist'] = fe_df['artist'].str.lower()\n",
    "fe_df['title'] = fe_df['title'].str.lower()\n",
    "fe_df['artist'] = fe_df['artist'].str.replace(' ','')\n",
    "fe_df['title'] = fe_df['title'].str.replace(' ','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('.','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('.','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('\\'','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('\\'','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('[','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('[','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace(']','')\n",
    "fe_df['title'] = fe_df['title'].str.replace(']','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('-','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('-','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('\\'','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('\\'','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('!','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('!','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('(','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('(','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace(')','')\n",
    "fe_df['title'] = fe_df['title'].str.replace(')','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace(',','')\n",
    "fe_df['title'] = fe_df['title'].str.replace(',','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('&','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('&','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('/','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('/','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('feat','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('feat','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('featuring','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('featuring','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('ft','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('ft','')\n",
    "fe_df['artist'] = fe_df['artist'].str.replace('ft.','')\n",
    "fe_df['title'] = fe_df['title'].str.replace('ft.','')\n",
    "\n",
    "# rename columns to match\n",
    "\n",
    "spotify_df.rename(columns={'name':'title'}, inplace=True)\n",
    "spotify_df.rename(columns={'artists':'artist'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes, while only showing the valid merges there should be no nulls\n",
    "merged_df = pd.merge(fe_df, spotify_df, on=['artist','title'], how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pring id feature of the merged dataframe\n",
    "print(merged_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes, while only showing the valid merges there should be no nulls\n",
    "spotify_orginal = pd.read_csv('spotify_data/train.csv')\n",
    "# i wanna get the original spotify data merged into the new dataframe based on id column, only add name and artists columns\n",
    "merged_df_newer = pd.merge(merged_df, spotify_orginal[['id','name','artists']], on='id', how='inner')\n",
    "merged_df_newer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make name and artists columns to be the frist and second columns\n",
    "cols = ['id']\n",
    "merged_df_newer = merged_df_newer[cols + [col for col in merged_df_newer.columns if col not in cols]]\n",
    "merged_df_newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop MSID, artist and title column\n",
    "merged_df_newer.drop(['MSDID','artist','title'], axis=1, inplace=True)\n",
    "merged_df_newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_newer.to_csv('merged_df_newer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_emb = merged_df_newer.copy()\n",
    "merged_df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I wanna embed the song name and artist name in the dataframe using Word2Vec\n",
    "merged_df_emb['artists'] = merged_df_emb['artists'].str.replace('[','')\n",
    "merged_df_emb['name'] = merged_df_emb['name'].str.replace('[','')\n",
    "merged_df_emb['artists'] = merged_df_emb['artists'].str.replace(']','')\n",
    "merged_df_emb['name'] = merged_df_emb['name'].str.replace(']','')\n",
    "merged_df_emb['artists'] = merged_df_emb['artists'].str.replace('\\'','')\n",
    "merged_df_emb['name'] = merged_df_emb['name'].str.replace('\\'','')\n",
    "merged_df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a simple encoding on names and artists\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "merged_df_emb['artists'] = le.fit_transform(merged_df_emb['artists'])\n",
    "merged_df_emb['name'] = le.fit_transform(merged_df_emb['name'])\n",
    "merged_df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = merged_df_emb.copy()\n",
    "clean_df.drop(['cleaned_text'], axis=1, inplace=True)\n",
    "clean_df.drop(['release_date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print any feature that has string values left\n",
    "for col in clean_df.columns:\n",
    "    if clean_df[col].dtype == 'object':\n",
    "        print(col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe with columns 2 till end of the dataframe\n",
    "clean_df = clean_df.iloc[:, 1:]\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if any row has string values\n",
    "for col in clean_df.columns:\n",
    "    if clean_df[col].dtype == 'object':\n",
    "        print(col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "non_categorical_columns = clean_df.columns\n",
    "df_non_categorical = clean_df[non_categorical_columns]\n",
    "norm_data = (df_non_categorical-df_non_categorical.min())/(df_non_categorical.max()-df_non_categorical.min())\n",
    "norm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back id column \n",
    "norm_data['id'] = merged_df_emb['id']\n",
    "norm_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data.to_csv('norm_data.csv', index=False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data_new = norm_data.copy()\n",
    "norm_data_new.drop(['id'], axis=1, inplace=True)\n",
    "norm_data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data_new.columns = norm_data_new.columns.astype(str)\n",
    "norm_data_new.columns\n",
    "\n",
    "# check if year has any string values\n",
    "for col in norm_data_new.columns:\n",
    "    if norm_data_new[col].dtype == 'object':\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA on the data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "principalComponents = pca.fit_transform(norm_data_new)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "principalDf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform SVD on the data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "svdComponents = svd.fit_transform(norm_data_new)\n",
    "svdDf = pd.DataFrame(data = svdComponents)\n",
    "svdDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge the PCA data with id column\n",
    "# principalDf['id'] = norm_data['id']\n",
    "# principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge the SVD data with id column\n",
    "# svdDf['id'] = norm_data['id']\n",
    "# svdDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make various plots and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the PCA data\n",
    "plt.scatter(principalDf[0], principalDf[1])\n",
    "plt.title('PCA')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()\n",
    "\n",
    "# plot the SVD data\n",
    "plt.scatter(svdDf[0], svdDf[1])\n",
    "plt.title('SVD')\n",
    "plt.xlabel('SVD 1')\n",
    "plt.ylabel('SVD 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Explained Variance for svd and plot on my data\n",
    "explained_variance = svd.explained_variance_ratio_\n",
    "explained_variance\n",
    "\n",
    "plt.plot(np.cumsum(explained_variance))\n",
    "plt.title('Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Explained Variance: PCA\n",
    "explained_variance_pca = pca.explained_variance_ratio_\n",
    "explained_variance_pca\n",
    "\n",
    "plt.plot(np.cumsum(explained_variance_pca))\n",
    "plt.title('Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highly_correlated_features(df, threshold):\n",
    "    correlation_matrix = df.corr() # Calculate the correlation matrix\n",
    "    \n",
    "    features_to_drop = set()\n",
    "    highly_correlated_pairs = []\n",
    "    \n",
    "    for i in range(correlation_matrix.shape[0]):\n",
    "        for j in range(i+1, correlation_matrix.shape[1]):\n",
    "            # If the absolute value of the correlation coefficient is above the threshold, add the feature to the set\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                features_to_drop.add(correlation_matrix.columns[j])\n",
    "                highly_correlated_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
    "\n",
    "    return features_to_drop , highly_correlated_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features, correlated_pairs  = highly_correlated_features(svdDf, 0.5)\n",
    "\n",
    "\n",
    "print(\"We can drop these Features:\",drop_features)\n",
    "print()\n",
    "print(\"Correlated Pairs and their coefficient\")\n",
    "for i in correlated_pairs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the features are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clsuters to do unsupervised learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Create a list to store the silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Create a list of different number of clusters to try\n",
    "n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Loop through each cluster\n",
    "for n in n_clusters:\n",
    "    # Create a KMeans object with n clusters and fit it to the data\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0)\n",
    "    kmeans.fit(svdDf)\n",
    "    \n",
    "    # Calculate the silhouette score and append it to the list\n",
    "    silhouette_scores.append(silhouette_score(svdDf, kmeans.labels_))\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.plot(n_clusters, silhouette_scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also do elbow method\n",
    "# Create a list to store the inertia values\n",
    "inertia_values = []\n",
    "\n",
    "# Loop through each cluster\n",
    "for n in n_clusters:\n",
    "    # Create a KMeans object with n clusters and fit it to the data\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0)\n",
    "    kmeans.fit(svdDf)\n",
    "    \n",
    "    # Append the inertia to the list\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the inertia values\n",
    "plt.plot(n_clusters, inertia_values)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Inertia vs Number of Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity with all the songs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "cosine_similarity_matrix_svd = cosine_similarity(svdDf)\n",
    "\n",
    "# Create a DataFrame for the cosine similarity matrix\n",
    "cosine_similarity_svd = pd.DataFrame(cosine_similarity_matrix_svd, index=svdDf.index, columns=svdDf.index)\n",
    "cosine_similarity_svd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_svd.to_csv('cosine_similarity_svd.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity with all the songs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "cosine_similarity_matrix_pca = cosine_similarity(principalDf)\n",
    "\n",
    "# Create a DataFrame for the cosine similarity matrix\n",
    "cosine_similarity_pca = pd.DataFrame(cosine_similarity_matrix_pca, index=principalDf.index, columns=principalDf.index)\n",
    "cosine_similarity_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get recommendations based on the cosine similarity matrix\n",
    "def get_recommendations(song_title,artist_name ,cosine_similarity_df, data):\n",
    "    # Get the index of the song based on title and artist\n",
    "    song_index = data[(data['name'] == song_title) & (data['artists'] == artist_name)].index[0]\n",
    "    # Get the similarity scores of the song\n",
    "    similarity_scores = cosine_similarity_df[song_index]\n",
    "    \n",
    "    # Get the indices of the songs with the highest similarity scores\n",
    "    similar_songs = similarity_scores.sort_values(ascending=False).index[1:4]\n",
    "    \n",
    "    # return like this: Index: 1, Name: 'song name', Artists: 'artist name'\n",
    "    recommended_songs = []\n",
    "    for i in similar_songs:\n",
    "        recommended_songs.append({'Index': i, 'Name': data['name'][i], 'Artists': data['artists'][i]})\n",
    "    \n",
    "    return recommended_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for the song 'SLovelight' by ABBA\n",
    "song, artist = 'Strawberry Swing','[\\'Coldplay\\']'\n",
    "recommendations = get_recommendations(song, artist,cosine_similarity_svd, merged_df_newer)\n",
    "print(f\"Since you listened to {song}, you might also like:\")\n",
    "for i in recommendations:\n",
    "    print('Index:', i['Index'], 'Name:', i['Name'], ' By:', i['Artists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wanna do RMSE on the results \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Create a function to calculate the RMSE\n",
    "def calculate_rmse(predictions, actual):\n",
    "    # Calculate the RMSE\n",
    "    rmse = sqrt(mean_squared_error(predictions, actual))\n",
    "    \n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for the song 'SLovelight' by ABBA\n",
    "song, artist = 'Strawberry Swing','[\\'Coldplay\\']'\n",
    "recommendations = get_recommendations(song,artist, cosine_similarity_pca, merged_df_newer)\n",
    "print(f\"Since you listened to {song}, you might also like:\")\n",
    "for i in recommendations:\n",
    "    print('Index:', i['Index'], 'Name:', i['Name'], ' By:', i['Artists'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataframe\n",
    "df_dup = pd.read_csv('merged_df_newer.csv')\n",
    "# I wanna remove duplicates where name and artists are the same\n",
    "df_dup.drop_duplicates(subset=['name','artists'], keep='first', inplace=True)\n",
    "df_dup.drop_duplicates(subset=['name','artists'], keep='first', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup.to_csv('merged_df_newer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataframe\n",
    "new_df = pd.read_csv('merged_df_newer.csv')\n",
    "# show data where artists name is 'Michael Jackson', show 'name'\n",
    "print(new_df[new_df['artists'] == '[\\'Red Hot Chili Peppers\\']']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
